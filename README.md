# NLP-task

## 项目简介

此项目是源于NEU NLP-Lab的暑期实训，主要包含了一些自然语言处理方面的工作。

`personal work`包含了一些关于NNLM的尝试，包括RNN、LSTM等经典模型和预训练词嵌入等新兴方法。此外还有一份阶段性报告。

`team work`中包含了一个基于BERT-base模型的情感分析项目，在实践时我们并没有一味地追求依靠模型规模来提升模型效果，而是把更多的精力放在了调优和创新预训练策略及方法上。

## BERT based sentiment classification model

这一项目旨在探索更高效的预训练方法，数据集由一份对话数据集改造而来，其中部分样本数据十分不均衡，对于模型来说是一个不小的挑战。

其次，我们不会一味地依靠大模型来提高它在数据集上的表现，而是争取让模型学得更系统的知识，从而以更小的推理成本换得更高的准确率。

在实践过程中，我们使用了多种classification head，在训练时采用了更细致的layer wise学习率和动态调度策略。

此外，我们还设计了基于对照学习的预训练任务，同时对样本使用预训练模型置换的方法进行数据增强。

最终我们成功地将模型表现从初始的`acc = 86%`提升到了`acc = 94%`，并且形成了一份研究文档：`NLP_Project_Summary.pdf`
